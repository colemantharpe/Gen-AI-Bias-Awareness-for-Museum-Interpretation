# HeritageTribalism_BigData

Software developed for the Futures Project in partial fulfilment of the requirements for the MSc Cultural Heritage Futures, The Univeristy of Edinburgh.

Title: Exploring Generative AI in Museum Interpretation: Balancing Innovation with Bias Awareness
Research question: How do Generative AI workflows (re)create biases in museum interpretation contexts?

This repository is the second output of the Futures Project and can be read in conjunction with the report. The directories associated with this repository contain the codes used for data collection and analysis, compiled as Jupyter Notebooks. This file describes the workflow used in the paper, which consists of 4 sections:

1. [Producing Interpretive Text with Generative AI](#producing-interpretive-text-with-generative-ai)
2. [Statistical Context](#statistical-context)
3. [Sentiment Analysis](#sentiment-analysis)
4. [Bias Detection](#bias-detection)

Each section briefly describes the analysis, linking to the code that was used to carry it out and produce the outputs. All the workbooks are located in the [codes](codes) directory. Additionally, the [data](data) directory contains the web metadata pertaining to the museum collection on display. The [images](images) directory contains the images associated with the museum collection. The [exports](exports) directory contains examples of files produced by the analysis that can be exported and taken away into different contexts. The [outputs](outputs) directory contains the outputs produced.

The numbers preceeding the names of the workbooks indicate the order in which they were executed, but each workbook is designed so that researchers and practitioners can work with different types of data in different order. As long as there are texts in a spreadsheet, then these workbooks can analyse the texts. 

Some of the workbooks contain figures produced from the analysis. Some of the figures are included in the Futures Project report and some are not. All of the figures are included here in this repository for reference. 

## 1 Producing interpretive text with Generative AI
To complete the process of collecting data to answer the research question outlined in the introduction, the [dataset](data/worldCulturesLivingLands.json) was extended to include interpretive texts produced by Generative AI.
In this [workbook](workbooks/1_generate_analysis.ipynb), the LLM is prompted with fine-tuned system instructions to create an entirely new interpretive text based on object metadata and a photograph. In order to achieve this, the methodology automates a complex yet reproducible task using the additional capabilities of the OpenAI Responses API. The Responses API includes several advanced functionalities, which benefit this methodology.
Firstly, the input syntax supports system-level guidance. In this workflow, system instructions allow for precise and consistent control over the model’s responses by separating the model’s role, task’s context, and response format specifications from the user prompt. The system instructions incorporate the local sector guidance on creating interpretive text along with a maximum length for the output. The system instructions are passed to the model with every API call, which instructs the model to take a consistent approach to each set of metadata passed to it. The system instructions are included in Appendix B: System instructions.
Secondly, the Responses API natively supports multi-media inputs, including the ability to prompt models with [images](images/). A model’s ability to process visual information from images is known as “Vision,” and only some models have this capability. This methodology uses the OpenAI ChatGPT 4o model specifically for its vision capability. Using images as inputs to prompts provides models with visual information that they can use in conjunction with other information such as object metadata to generate a response. In practice, this workflow passes the system instructions to the model along with the object metadata and a Base64-encoded data URL of the image of the object. The API then returns a response which includes the interpretive text for the object along with the number of tokens used in generating that response. These are added to the [dataset](/outputs/GenAIinterpretation.csv) for further analysis.

## 2 Statistical context
The methodology to this point has created a [dataset of interpretive texts](outputs/GenAIinterpretation.csv) created by the museum and created by Generative AI. To compare the texts, the TextStat utility was used to calculate base metrics and readability for both sets of texts. TextStat is a Python library, which contains a suite of single-line functions for calculating quantitative data about text including but not limited to word count, sentence count, and average sentence length. It also includes a range of functions which calculate readability by assigning a score against a scale or by assigning a school-grade reading level to a particular text.
The [workbook](workbooks/2_analysis.ipynb) outlines the functions by which the following statistics were calculated for both sets of interpretive texts: word count, sentence count, Flesch Reading Ease measure, and McAlpine-EFLAW Index. The counts were collected to compare the structure of the two sets of text at a very high level. The Flesch Reading Ease measure, which emerged as a standard method of determining readability in the 1940s, applies an equation which incorporates number of syllables and average number of words per sentence to determine a score from 0 to 100. In this formula, long sentences and long words decrease the text’s readability. On this scale, higher scores indicate texts that are easier to read. The McAlpine-EFLAW Index takes a different approach. Instead of problematising long words and long sentences, which generally cause problems for native English speakers, the McAlpine-EFLAW Index evaluates the number of words of one, two, or three letters (“miniwords”) in a sentence. In this equation, higher proportions of miniwords decrease readability. On this scale, lower scores indicate texts that are easier to read.
Because the two readability metrics described here take different approaches, using them together provides a broader base of analysis. Their differing approaches were designed to address the needs of two non-overlapping audiences–native English users and users of English as an additional language. All of the text in the gallery and all of the text produced by Generative AI is written in English, but Scotland received 4.4 million international visitors in the year 2024. Evaluating the readability of the language of those texts is the first step in evaluating the potential biases contained therein. The statstics for each text in the dataset are saved to a new [output file](outputs/textStatistics.csv) for further analysis.

## 3 Sentiment Analysis
The second method of exploration is [sentiment analysis](workbooks/3_sentiment.ipynb). Unlike the statistical analysis described in the previous section, sentiment analysis is an NLP approach which aims to predict human emotion in text. Sentiment analysis frameworks have been applied to a wide range of use cases in not only research but also business intelligence and have successfully predicted human sentiment across both structured and unstructured texts in contexts as diverse as online product reviews, social media posts, and news. Methods fall into one of three broad groups: dictionary-based methods, machine learning methods, or multimodal methods combining pre-defined dictionaries with machine learning models. This methodology incorporates sentiment analysis by both a machine learning method (FLAIR) and by a dictionary-based method (VADAR) to present a wider view on the concept of sentiment in interpretive texts. Algorithmic assessments of the sentiment scores for each text are recorded to a new [output file](outputs/textSentiment.csv) for further analysis.

## 4 Bias detection
The third and final area of exploration related to the research question outlined in the introduction is that which is most closely related to the creation and recreation of biases in the interpretive texts. This section of the methodology uses NLP methods to detect and classify a range of potential issues in text across multiple different categories of social bias. As in the example above describing the potential methods for sentiment analysis, bias detection using NLP falls into one of the following broad approaches: corpus analysis, lexicon-based tagging, machine learning embedding, fairness metrics, and qualitative contextual analysis. As in the workflow exploring [sentiment analysis](workbooks/3_sentiment.ipynb), this methodology uses both a machine-learning approach and a lexicon-based approach to present a range of tools available to explore bias in text. This workflow is detailed across two workbooks.

###	GUS-Net
The GUS framework moves beyond binary classification methods. The model uses pre-trained Bidirectional Encoder Representations from Transformers (BERT) to generate multi-label token classification across three fundamental linguistic components underlying social bias in text: Generalisations, Unfairness, and Stereotypes (the GUS framework). The GUS NER toolkit is an emerging resource for social bias detection, which presents advantages over previous models in its granularity–tagging of individual tokens–and in its flexibility–labelling individual tokens as potentially problematic along multiple axes of bias.
The GUS framework for bias detection is available through multiple pathways. Firstly, the token-level annotations framework is available as GUS-Net, a package of modules and libraries from Hugging Face, as well as in a standalone, browser-based demo. Secondly, one of the principal programmers has made available a bias type classifier, a related tool that aggregates the probabilities of different types of bias in a particular text across eleven pre-defined social bias categories based on contextualised word embeddings. The GUS-Net social bias NER toolkit is an emerging tool for social bias detection in text, which outperforms unrefined encoding models and pure LLM-based models on bias detection tasks. It is a powerful pre-trained and fine-tuned dataset that works across a variety of different contexts without requiring significant computing power.
The [workbook](workbooks/4_GUS-Net.ipynb) outlines includes both pathways mentioned above and their [outputs](outputs/gusnet.csv) are saved to a single file. The workflow first uses the probabilistic bias type classifier to identify the most likely potentially problematic texts then the token-level annotations framework to label individual words which may be introducing biased language. Token-level results can be [exported](exports/A.1909.499.32.csv) from the workbook and analysed separately. This two-phased approach is meant to be empowering to researchers and practitioners looking to develop human-in-the-loop processes to identify areas of potential bias for further investigation and mitigation.

###	DE-BIAS
The final tool is the first context-specific tool outlined in this methodology. The [DE-BIAS Tool](workbooks/5_DE-BIAS.ipynb) is a technical output of the DE-BIAS project, a two-year project to develop and promote a more inclusive approach to describing digital collections coordinated by the Common European Data Space for Cultural Heritage. It combines an annotated library with LLM-based filtering to identify context-specific examples of bias in texts. The vocabulary’s thematic focus is specific to the cultural heritage sector including such themes as migration and colonial history, gender and sexual identity, ethnicity and ethno-religious identity, and other forms of minoritisation within overlapping matricies of power. Further, this vocabulary was developed with input from minoritised communities and cites over 100 existing glossaries and academic publications. 
This methodology accesses the DE-BIAS tool through the public API, but researchers and practitioners can also use the tool through a browser-based demo. Both pathways accept text in five different languages (Dutch, English, French, German, and Italian) and return lists of potentially problematic words from the vocabulary along with potential issues relating to those particular words and a source relating to the model’s identification of that particular issue. Similar to the GUS-Net framework described in the previous section, the DE-BIAS tool returns actionable bias annotations at a granular level, which can be [exported](exports/debias_filtered.csv) for further discussion. Although currently a small vocabulary of about 700 words, the DE-BIAS Tool has been developed within the European cultural heritage context and provides context-specific information around potential biases to inform human-in-the-loop bias mitigation strategies.
